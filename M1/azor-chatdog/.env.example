ENGINE=GEMINI
GEMINI_API_KEY="ðŸ¥²-SIEMANKO-TU-API-KEY-ðŸ¥²"
MODEL_NAME="gemini-2.5-flash"

# ENGINE=LLAMA_CPP
# MODEL_NAME=gemma-2-2b-it-Q8_0
#
# # ÅšcieÅ¼ka do modelu GGUF - przykÅ‚adowe modele:
# # Szybki i lekki (2.3GB RAM):
# # LLAMA_MODEL_PATH=~/models/gemma-2-2b-it-Q8_0.gguf
# #
# # Mocniejszy (5GB RAM):
# # LLAMA_MODEL_PATH=~/models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
#
# LLAMA_MODEL_PATH=/TWOJA_SCIEZKA_DO_MODELU/nazwa_modelu.gguf
# LLAMA_GPU_LAYERS=-1
#
# # Rozmiar kontekstu:
# # Gemma 2 2B: zalecane 4096-8192, max 8192
# # Llama 3.1 8B: zalecane 4096-8192, max 32768
# LLAMA_CONTEXT_SIZE=8192
#
# # Parametry generowania dla llama-cpp
# LLAMA_TEMPERATURE=0.7
# LLAMA_TOP_P=0.9
# LLAMA_TOP_K=40
